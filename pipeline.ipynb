{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from helper import *\n",
    "import ast\n",
    "from fastcoref import FCoref\n",
    "import tqdm\n",
    "\n",
    "#extracting dataframe paths\n",
    "parent_folder = os.path.dirname(os.path.dirname(os.path.abspath(\"alt2.ipynb\")))\n",
    "char_data_path= os.path.join(parent_folder, \"Data\\\\character.metadata.tsv\")\n",
    "plot_data_path= os.path.join(parent_folder, \"Data\\\\resolved_texts_fastcoref.csv\")\n",
    "\n",
    "#building plot summaries dataframe\n",
    "ind={0:\"Wikipedia movie ID\", 1:\"Freebase movie ID\", 2:\"Movie release date\", 3:\"Character name\", 4:\"Actor date of birth\", 5:\"Actor gender\", 6:\"Actor height\", 7:\"Actor ethnicity\", 8:\"Actor name\", 9:\"Actor age at movie release\", 10:\"Freebase character/actor map ID\", 11:\"Freebase character ID\", 12:\"Freebase actor ID\"}\n",
    "characters_df= pd.read_csv(char_data_path, delimiter=\"\\t\", header=None)\n",
    "characters_df=characters_df.rename(columns=ind)\n",
    "PLOT_SUMMARY_PATH = \"..\\\\data\\\\plot_summaries.txt\"\n",
    "plot_summaries = pd.read_csv(PLOT_SUMMARY_PATH, sep='\\t', header=None)\n",
    "plot_summaries.columns = ['wiki_id', 'plot']\n",
    "movies = pd.read_csv(os.path.join(parent_folder, \"Data\\\\movie.metadata.tsv\"), sep='\\t', header=None)\n",
    "movies.columns = [\n",
    "    'Wikipedia movie ID',\n",
    "    'freebase_movie_id',\n",
    "    'movie_name',\n",
    "    'movie_release_date',\n",
    "    'movie_box_office_revenue',\n",
    "    'movie_runtime',\n",
    "    'movie_languages',\n",
    "    'movie_countries',\n",
    "    'movie_genres'\n",
    "]\n",
    "movies['year'] = movies['movie_release_date'].str.extract('(\\d{4})', expand=False)\n",
    "movies['year'] = pd.to_numeric(movies['year'], downcast='integer')\n",
    "movies['movie_box_office_revenue'] = pd.to_numeric(movies['movie_box_office_revenue'], errors='coerce')\n",
    "movies['movie_runtime'] = pd.to_numeric(movies['movie_runtime'], errors='coerce')\n",
    "# movie language distribution\n",
    "query = re.compile(r'\"(\\w+) Language\"')\n",
    "movies['movie_languages'] = movies['movie_languages'].apply(lambda x: query.findall(x))\n",
    "query = re.compile(r': \"(.+)\"')\n",
    "movies['movie_countries'] = movies['movie_countries'].apply(lambda x: query.findall(x)[0] if query.findall(x) != [] else '')\n",
    "query = re.compile(r': \"(.+?)\"')\n",
    "movies['movie_genres'] = movies['movie_genres'].apply(lambda x: query.findall(x))\n",
    "characters_df_full= pd.read_csv(os.path.join(parent_folder, \"Data\\\\character.metadata.tsv\"), sep='\\t', header=None)\n",
    "characters_df_full.columns = [\n",
    "    'Wikipedia movie ID',\n",
    "    'freebase_movie_id',\n",
    "    'movie_release_date',\n",
    "    'character_name',\n",
    "    'actor_dob',\n",
    "    'actor_gender',\n",
    "    'actor_height',\n",
    "    'actor_ethnicity',\n",
    "    'actor_name',\n",
    "    'actor_age',\n",
    "    'freebase_character_map_1',\n",
    "    'freebase_character_map_2',\n",
    "    'freebase_character_map_3'\n",
    "]\n",
    "characters_df=characters_df_full[['Wikipedia movie ID','character_name', 'actor_dob','actor_gender','actor_height', 'actor_ethnicity', 'actor_name', 'actor_age', 'freebase_character_map_1', 'freebase_character_map_2','freebase_character_map_3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet performs the coreference on the plots and returns the resolved texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_texts = []\n",
    "model = FCoref(device='cuda:0')\n",
    "characters = characters_df['character_name'].unique().tolist()\n",
    "characters = [name for name in characters if type(name) == str]\n",
    "characters = ' '.join(characters)\n",
    "\n",
    "for idx, row in plot_summaries.iterrows():\n",
    "    text = row['plot']\n",
    "    text_split = text.split()\n",
    "    resolved_text = text.split()\n",
    "    preds = model.predict(\n",
    "        texts=text.split(),\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    clusters = preds.get_clusters(as_strings=False)\n",
    "    for cluster in clusters:\n",
    "        character_name = None\n",
    "        for token_offset in cluster:\n",
    "            token = ' '.join(text_split[token_offset[0]:token_offset[1]])\n",
    "            if token in characters and token.lower() not in [\"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"they\", \"them\", \"their\"]:\n",
    "                character_name = token\n",
    "            elif token.lower() in [\"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"they\", \"them\", \"their\"] and character_name!=None:\n",
    "                resolved_text[token_offset[0]] = character_name\n",
    "    resolved_texts.append(' '.join(resolved_text))\n",
    "\n",
    "wiki_ids = plot_summaries['wiki_id'].tolist()\n",
    "resolved_df = pd.DataFrame({'wiki_id': wiki_ids, 'resolved_text': resolved_texts})\n",
    "resolved_df.to_csv('../data/resolved_texts_fastcoref.csv', index=False)\n",
    "plots_df = pd.read_csv(plot_data_path, delimiter=',')\n",
    "plots_df = plots_df.rename(columns={\"wiki_id\":\"Wikipedia movie ID\", \"resolved_text\":\"Plot Summary\"})\n",
    "plots_df=plots_df.set_index(\"Wikipedia movie ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code snippet was used to create Verb_Subject_Object.csv and character_list.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.DataFrame(columns=[\"Wikipedia movie ID\", \"Sentence\", \"Verb\", \"Subject\", \"Object\"])\n",
    "char_df=pd.DataFrame(columns=[\"Wikipedia movie ID\", \"characters\"])\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "for i, id in enumerate(plots_df.index):\n",
    "    sent_df=pd.DataFrame(columns=[\"Wikipedia movie ID\", \"Sentence\", \"Verb\", \"Subject\", \"Object\", \"nsubj\", \"nsubjpass\", \"dobj\", \"agent\", \"ccomp\"])\n",
    "    plot=plots_df.iloc[i][\"Plot Summary\"]\n",
    "    doc= nlp(plot)\n",
    "    characters = get_characters(doc)\n",
    "    print(characters)\n",
    "    char_values = {\"Wikipedia movie ID\": id,\"characters\": characters}\n",
    "    char_df.loc[len(char_df)] = char_values\n",
    "    sent_num = 0\n",
    "    for sent in doc.sents:\n",
    "        # print the verb and all its children and their dependency relations\n",
    "        sent_num += 1\n",
    "        for token in sent:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                # Create a dictionary with the values to be assigned\n",
    "                values = {\"Wikipedia movie ID\": id,\"Sentence\": sent_num, \"Verb\": token.lemma_}\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in sent_df.columns:\n",
    "                        values[child.dep_] = get_all_children(child)\n",
    "                # Append the dictionary as a new row to sent_df\n",
    "                sent_df.loc[len(sent_df)] = values\n",
    "    sent_df[\"Object\"] = sent_df[\"dobj\"].combine_first(sent_df[\"nsubjpass\"]).combine_first(sent_df[\"ccomp\"])\n",
    "    sent_df.drop(columns=[\"dobj\", \"nsubjpass\", \"ccomp\"], inplace=True)\n",
    "    sent_df[\"Subject\"] = sent_df[\"nsubj\"].combine_first(sent_df[\"agent\"])\n",
    "    sent_df.drop(columns=[\"nsubj\", \"agent\"], inplace=True)\n",
    "    final_df=pd.concat([final_df, sent_df], ignore_index=True)\n",
    "    if (i+1) % 5000 == 0:\n",
    "        final_df.to_csv(f'Verb_Subject_Object_{i}.csv', index=False)\n",
    "        char_df.to_csv(f'characters_{i}.csv', index=False)\n",
    "        char_df=pd.DataFrame(columns=[\"Wikipedia movie ID\", \"characters\"])\n",
    "        final_df = pd.DataFrame(columns=[\"Wikipedia movie ID\", \"Sentence\", \"Verb\", \"Subject\", \"Object\"])\n",
    "    print(\"Done with movie : \", i)\n",
    "\n",
    "char_df.to_csv('Data\\\\characters_end.csv', index=False)\n",
    "final_df.to_csv('Data\\\\Verb_Subject_Object_end.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code snippet was used to create charA_action_charB.csv and graph_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df.apply(lambda row: (isinstance(row[\"Subject\"], list) and any(elem in row[\"characters\"] for elem in row[\"Subject\"])) or (isinstance(row[\"Object\"], list) and any(elem in row[\"characters\"] for elem in row[\"Object\"])), axis=1)]\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def transform_row(row):\n",
    "    verb = [row[\"Verb\"]] if pd.notna(row[\"Verb\"]) else []\n",
    "    subject = row[\"Subject\"] if isinstance(row[\"Subject\"], list) else []\n",
    "    obj = row[\"Object\"] if isinstance(row[\"Object\"], list) else []\n",
    "    characters= row[\"characters\"] if isinstance(row[\"characters\"], list) else []\n",
    "    char_A = set([char for char in subject if  char in characters])\n",
    "    char_B = set([char for char in obj if char in characters])\n",
    "    action = []\n",
    "    for word in (verb + obj):\n",
    "        if word in char_A:\n",
    "            continue\n",
    "        elif word in characters:\n",
    "            break\n",
    "        action.append(word)\n",
    "    print(\"done with index\", row.name)\n",
    "    return pd.Series({\"Wikipedia movie ID\": row[\"Wikipedia movie ID\"],\"char A\": list(char_A), \"action\": action, \"char B\": list(char_B)})\n",
    "\n",
    "transformed_df = filtered_df.apply(transform_row, axis=1)\n",
    "graph_df=transformed_df[(transformed_df[\"char A\"].apply(lambda x: len(x) > 0)) & (transformed_df[\"char B\"].apply(lambda x: len(x) > 0)) & (transformed_df[\"char A\"]!=transformed_df[\"char B\"])]\n",
    "graph_df=graph_df.explode('char B').explode('char A')\n",
    "graph_df=graph_df[graph_df['char A']!=graph_df['char B']].reset_index(drop=True)\n",
    "graph_df['combined_action'] = graph_df['action'].apply(lambda x: ' '.join(x))\n",
    "transformed_df.to_csv('Data\\\\charA_action_charB.csv', index=False)  \n",
    "graph_df.to_csv('Data\\\\graph_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code snippet is used to create final_merged_df.csv which the main dataframe we used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "vocab_raw = set(graph_df['combined_action'].str.split().explode())\n",
    "\n",
    "vocab_filtered = {word.lower() for word in vocab_raw if word.isalpha() and not word.isupper()}\n",
    "\n",
    "lemmatizer = {word: token.lemma_ for word in vocab_filtered for token in nlp(word)}\n",
    "\n",
    "word_vectors = {word: nlp(word).vector for word in vocab}\n",
    "def replace_chars(row):\n",
    "    characters= characters_df[characters_df['Wikipedia movie ID']==row['Wikipedia movie ID']]['character_name'].dropna().tolist()\n",
    "    if not characters:\n",
    "        return [row['char A'], row['char B']]\n",
    "    i=0\n",
    "    char_A_name=\"\"\n",
    "    char_B_name=\"\"\n",
    "    for chars in characters:\n",
    "        if row['char A'] in chars:\n",
    "            char_A_name = chars\n",
    "            i+=1\n",
    "        if row['char B'] in chars:\n",
    "            char_B_name = chars\n",
    "            i+=1\n",
    "        if i==2:\n",
    "            print(\"Done with row: \", row.name)\n",
    "            return [char_A_name, char_B_name]\n",
    "    if char_A_name==\"\":\n",
    "        char_A_name = row['char A']\n",
    "    if char_B_name==\"\":\n",
    "        char_B_name = row['char B']\n",
    "    print(\"Done with row: \", row.name)\n",
    "    return [char_A_name, char_B_name]\n",
    "\n",
    "#lemmatize the action column\n",
    "def lemmatize_action(action):\n",
    "    return [lemmatizer.get(word, word) for word in action]\n",
    "\n",
    "# Apply the lemmatizer function to create a new column 'lemmatized_action'\n",
    "graph_df['lemmatized_action'] = graph_df['action'].apply(lemmatize_action)\n",
    "graph_df=graph_df.merge(characters_df[['Wikipedia movie ID', 'character', 'character_type']], how='left', left_on=['Wikipedia movie ID', 'char A'] , right_on=['Wikipedia movie ID', 'character'])\n",
    "graph_df=graph_df.rename(columns={\"character_type\":\"char A type\"}).drop(columns=['character'])\n",
    "graph_df=graph_df.merge(characters_df[['Wikipedia movie ID', 'character', 'character_type']], how='left', left_on=['Wikipedia movie ID', 'char B'] , right_on=['Wikipedia movie ID', 'character'])\n",
    "graph_df=graph_df.rename(columns={\"character_type\":\"char B type\"}).drop(columns=['character'])\n",
    "graph_df['char A type'].fillna('side', inplace=True)\n",
    "graph_df['char B type'].fillna('side', inplace=True)\n",
    "temp_df=graph_df.apply(lambda row: replace_chars(row), axis=1)\n",
    "real_char=pd.DataFrame(temp_df)\n",
    "real_char[['charA', 'charB']] = real_char[0].apply(pd.Series)\n",
    "real_char.drop(0, axis=1, inplace=True)\n",
    "graph_df[['char A', 'char B']] = real_char[['charA', 'charB']]\n",
    "graph_df = graph_df.merge(movies, how='left', on='Wikipedia movie ID', suffixes=('_graph', '_movies'))\n",
    "graph_df = graph_df.merge(characters_df, how='left', left_on=['Wikipedia movie ID', 'char A'], right_on=['Wikipedia movie ID', 'character_name'], suffixes=('', '_charA'))\n",
    "graph_df.columns = [f'{col}_charA' if col in characters_df.columns and col != 'Wikipedia movie ID' else col for col in graph_df.columns]\n",
    "graph_df = graph_df.merge(characters_df, how='left', left_on=['Wikipedia movie ID', 'char B'], right_on=['Wikipedia movie ID', 'character_name'], suffixes=('', '_charB'))\n",
    "graph_df.columns = [f'{col}_charB' if col in characters_df.columns and col != 'Wikipedia movie ID' else col for col in graph_df.columns]\n",
    "graph_df.drop_duplicates(['Wikipedia movie ID', 'char A', 'combined_action', 'char B'], inplace=True)\n",
    "graph_df.to_csv('Data\\\\final_merged_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
