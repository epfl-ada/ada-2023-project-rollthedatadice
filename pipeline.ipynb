{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Loading\" data-toc-modified-id=\"Data-Loading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Loading</a></span></li><li><span><a href=\"#Pipeline\" data-toc-modified-id=\"Pipeline-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pipeline</a></span></li><li><span><a href=\"#clustering-interaactions-in-different-topics\" data-toc-modified-id=\"clustering-interaactions-in-different-topics-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>clustering interaactions in different topics</a></span></li><li><span><a href=\"#Graph-drawing\" data-toc-modified-id=\"Graph-drawing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Graph drawing</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from helper import *\n",
    "import ast\n",
    "from fastcoref import FCoref\n",
    "import tqdm\n",
    "\n",
    "#extracting dataframe paths\n",
    "parent_folder = os.path.dirname(os.path.dirname(os.path.abspath(\"alt2.ipynb\")))\n",
    "char_data_path= os.path.join(parent_folder, \"Data\\\\character.metadata.tsv\")\n",
    "plot_data_path= os.path.join(parent_folder, \"Data\\\\resolved_texts_fastcoref.csv\")\n",
    "\n",
    "#building plot summaries dataframe\n",
    "ind={0:\"Wikipedia movie ID\", 1:\"Freebase movie ID\", 2:\"Movie release date\", 3:\"Character name\", 4:\"Actor date of birth\", 5:\"Actor gender\", 6:\"Actor height\", 7:\"Actor ethnicity\", 8:\"Actor name\", 9:\"Actor age at movie release\", 10:\"Freebase character/actor map ID\", 11:\"Freebase character ID\", 12:\"Freebase actor ID\"}\n",
    "characters_df= pd.read_csv(char_data_path, delimiter=\"\\t\", header=None)\n",
    "characters_df=characters_df.rename(columns=ind)\n",
    "PLOT_SUMMARY_PATH = \"..\\\\data\\\\plot_summaries.txt\"\n",
    "plot_summaries = pd.read_csv(PLOT_SUMMARY_PATH, sep='\\t', header=None)\n",
    "plot_summaries.columns = ['wiki_id', 'plot']\n",
    "movies = pd.read_csv(os.path.join(parent_folder, \"Data\\\\movie.metadata.tsv\"), sep='\\t', header=None)\n",
    "movies.columns = [\n",
    "    'Wikipedia movie ID',\n",
    "    'freebase_movie_id',\n",
    "    'movie_name',\n",
    "    'movie_release_date',\n",
    "    'movie_box_office_revenue',\n",
    "    'movie_runtime',\n",
    "    'movie_languages',\n",
    "    'movie_countries',\n",
    "    'movie_genres'\n",
    "]\n",
    "movies['year'] = movies['movie_release_date'].str.extract('(\\d{4})', expand=False)\n",
    "movies['year'] = pd.to_numeric(movies['year'], downcast='integer')\n",
    "movies['movie_box_office_revenue'] = pd.to_numeric(movies['movie_box_office_revenue'], errors='coerce')\n",
    "movies['movie_runtime'] = pd.to_numeric(movies['movie_runtime'], errors='coerce')\n",
    "# movie language distribution\n",
    "query = re.compile(r'\"(\\w+) Language\"')\n",
    "movies['movie_languages'] = movies['movie_languages'].apply(lambda x: query.findall(x))\n",
    "query = re.compile(r': \"(.+)\"')\n",
    "movies['movie_countries'] = movies['movie_countries'].apply(lambda x: query.findall(x)[0] if query.findall(x) != [] else '')\n",
    "query = re.compile(r': \"(.+?)\"')\n",
    "movies['movie_genres'] = movies['movie_genres'].apply(lambda x: query.findall(x))\n",
    "characters_df_full= pd.read_csv(os.path.join(parent_folder, \"Data\\\\character.metadata.tsv\"), sep='\\t', header=None)\n",
    "characters_df_full.columns = [\n",
    "    'Wikipedia movie ID',\n",
    "    'freebase_movie_id',\n",
    "    'movie_release_date',\n",
    "    'character_name',\n",
    "    'actor_dob',\n",
    "    'actor_gender',\n",
    "    'actor_height',\n",
    "    'actor_ethnicity',\n",
    "    'actor_name',\n",
    "    'actor_age',\n",
    "    'freebase_character_map_1',\n",
    "    'freebase_character_map_2',\n",
    "    'freebase_character_map_3'\n",
    "]\n",
    "characters_df=characters_df_full[['Wikipedia movie ID','character_name', 'actor_dob','actor_gender','actor_height', 'actor_ethnicity', 'actor_name', 'actor_age', 'freebase_character_map_1', 'freebase_character_map_2','freebase_character_map_3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet performs the coreference on the plots and returns the resolved texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_texts = []\n",
    "model = FCoref(device='cuda:0')\n",
    "characters = characters_df['character_name'].unique().tolist()\n",
    "characters = [name for name in characters if type(name) == str]\n",
    "characters = ' '.join(characters)\n",
    "\n",
    "for idx, row in plot_summaries.iterrows():\n",
    "    text = row['plot']\n",
    "    text_split = text.split()\n",
    "    resolved_text = text.split()\n",
    "    preds = model.predict(\n",
    "        texts=text.split(),\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    clusters = preds.get_clusters(as_strings=False)\n",
    "    for cluster in clusters:\n",
    "        character_name = None\n",
    "        for token_offset in cluster:\n",
    "            token = ' '.join(text_split[token_offset[0]:token_offset[1]])\n",
    "            if token in characters and token.lower() not in [\"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"they\", \"them\", \"their\"]:\n",
    "                character_name = token\n",
    "            elif token.lower() in [\"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"they\", \"them\", \"their\"] and character_name!=None:\n",
    "                resolved_text[token_offset[0]] = character_name\n",
    "    resolved_texts.append(' '.join(resolved_text))\n",
    "\n",
    "wiki_ids = plot_summaries['wiki_id'].tolist()\n",
    "resolved_df = pd.DataFrame({'wiki_id': wiki_ids, 'resolved_text': resolved_texts})\n",
    "resolved_df.to_csv('../data/resolved_texts_fastcoref.csv', index=False)\n",
    "plots_df = pd.read_csv(plot_data_path, delimiter=',')\n",
    "plots_df = plots_df.rename(columns={\"wiki_id\":\"Wikipedia movie ID\", \"resolved_text\":\"Plot Summary\"})\n",
    "plots_df=plots_df.set_index(\"Wikipedia movie ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code snippet was used to create Verb_Subject_Object.csv and character_list.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.DataFrame(columns=[\"Wikipedia movie ID\", \"Sentence\", \"Verb\", \"Subject\", \"Object\"])\n",
    "char_df=pd.DataFrame(columns=[\"Wikipedia movie ID\", \"characters\"])\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "for i, id in enumerate(plots_df.index):\n",
    "    sent_df=pd.DataFrame(columns=[\"Wikipedia movie ID\", \"Sentence\", \"Verb\", \"Subject\", \"Object\", \"nsubj\", \"nsubjpass\", \"dobj\", \"agent\", \"ccomp\"])\n",
    "    plot=plots_df.iloc[i][\"Plot Summary\"]\n",
    "    doc= nlp(plot)\n",
    "    characters = get_characters(doc)\n",
    "    print(characters)\n",
    "    char_values = {\"Wikipedia movie ID\": id,\"characters\": characters}\n",
    "    char_df.loc[len(char_df)] = char_values\n",
    "    sent_num = 0\n",
    "    for sent in doc.sents:\n",
    "        # print the verb and all its children and their dependency relations\n",
    "        sent_num += 1\n",
    "        for token in sent:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                # Create a dictionary with the values to be assigned\n",
    "                values = {\"Wikipedia movie ID\": id,\"Sentence\": sent_num, \"Verb\": token.lemma_}\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in sent_df.columns:\n",
    "                        values[child.dep_] = get_all_children(child)\n",
    "                # Append the dictionary as a new row to sent_df\n",
    "                sent_df.loc[len(sent_df)] = values\n",
    "    sent_df[\"Object\"] = sent_df[\"dobj\"].combine_first(sent_df[\"nsubjpass\"]).combine_first(sent_df[\"ccomp\"])\n",
    "    sent_df.drop(columns=[\"dobj\", \"nsubjpass\", \"ccomp\"], inplace=True)\n",
    "    sent_df[\"Subject\"] = sent_df[\"nsubj\"].combine_first(sent_df[\"agent\"])\n",
    "    sent_df.drop(columns=[\"nsubj\", \"agent\"], inplace=True)\n",
    "    final_df=pd.concat([final_df, sent_df], ignore_index=True)\n",
    "    if (i+1) % 5000 == 0:\n",
    "        final_df.to_csv(f'Verb_Subject_Object_{i}.csv', index=False)\n",
    "        char_df.to_csv(f'characters_{i}.csv', index=False)\n",
    "        char_df=pd.DataFrame(columns=[\"Wikipedia movie ID\", \"characters\"])\n",
    "        final_df = pd.DataFrame(columns=[\"Wikipedia movie ID\", \"Sentence\", \"Verb\", \"Subject\", \"Object\"])\n",
    "    print(\"Done with movie : \", i)\n",
    "\n",
    "char_df.to_csv('Data\\\\characters_end.csv', index=False)\n",
    "final_df.to_csv('Data\\\\Verb_Subject_Object_end.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code snippet was used to create charA_action_charB.csv and graph_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df.apply(lambda row: (isinstance(row[\"Subject\"], list) and any(elem in row[\"characters\"] for elem in row[\"Subject\"])) or (isinstance(row[\"Object\"], list) and any(elem in row[\"characters\"] for elem in row[\"Object\"])), axis=1)]\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def transform_row(row):\n",
    "    verb = [row[\"Verb\"]] if pd.notna(row[\"Verb\"]) else []\n",
    "    subject = row[\"Subject\"] if isinstance(row[\"Subject\"], list) else []\n",
    "    obj = row[\"Object\"] if isinstance(row[\"Object\"], list) else []\n",
    "    characters= row[\"characters\"] if isinstance(row[\"characters\"], list) else []\n",
    "    char_A = set([char for char in subject if  char in characters])\n",
    "    char_B = set([char for char in obj if char in characters])\n",
    "    action = []\n",
    "    for word in (verb + obj):\n",
    "        if word in char_A:\n",
    "            continue\n",
    "        elif word in characters:\n",
    "            break\n",
    "        action.append(word)\n",
    "    print(\"done with index\", row.name)\n",
    "    return pd.Series({\"Wikipedia movie ID\": row[\"Wikipedia movie ID\"],\"char A\": list(char_A), \"action\": action, \"char B\": list(char_B)})\n",
    "\n",
    "transformed_df = filtered_df.apply(transform_row, axis=1)\n",
    "graph_df=transformed_df[(transformed_df[\"char A\"].apply(lambda x: len(x) > 0)) & (transformed_df[\"char B\"].apply(lambda x: len(x) > 0)) & (transformed_df[\"char A\"]!=transformed_df[\"char B\"])]\n",
    "graph_df=graph_df.explode('char B').explode('char A')\n",
    "graph_df=graph_df[graph_df['char A']!=graph_df['char B']].reset_index(drop=True)\n",
    "graph_df['combined_action'] = graph_df['action'].apply(lambda x: ' '.join(x))\n",
    "transformed_df.to_csv('Data\\\\charA_action_charB.csv', index=False)  \n",
    "graph_df.to_csv('Data\\\\graph_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code snippet is used to create final_merged_df.csv which is the main dataframe we used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "vocab_raw = set(graph_df['combined_action'].str.split().explode())\n",
    "\n",
    "vocab_filtered = {word.lower() for word in vocab_raw if word.isalpha() and not word.isupper()}\n",
    "\n",
    "lemmatizer = {word: token.lemma_ for word in vocab_filtered for token in nlp(word)}\n",
    "\n",
    "word_vectors = {word: nlp(word).vector for word in vocab}\n",
    "def replace_chars(row):\n",
    "    characters= characters_df[characters_df['Wikipedia movie ID']==row['Wikipedia movie ID']]['character_name'].dropna().tolist()\n",
    "    if not characters:\n",
    "        return [row['char A'], row['char B']]\n",
    "    i=0\n",
    "    char_A_name=\"\"\n",
    "    char_B_name=\"\"\n",
    "    for chars in characters:\n",
    "        if row['char A'] in chars:\n",
    "            char_A_name = chars\n",
    "            i+=1\n",
    "        if row['char B'] in chars:\n",
    "            char_B_name = chars\n",
    "            i+=1\n",
    "        if i==2:\n",
    "            print(\"Done with row: \", row.name)\n",
    "            return [char_A_name, char_B_name]\n",
    "    if char_A_name==\"\":\n",
    "        char_A_name = row['char A']\n",
    "    if char_B_name==\"\":\n",
    "        char_B_name = row['char B']\n",
    "    print(\"Done with row: \", row.name)\n",
    "    return [char_A_name, char_B_name]\n",
    "\n",
    "#lemmatize the action column\n",
    "def lemmatize_action(action):\n",
    "    return [lemmatizer.get(word, word) for word in action]\n",
    "\n",
    "# Apply the lemmatizer function to create a new column 'lemmatized_action'\n",
    "graph_df['lemmatized_action'] = graph_df['action'].apply(lemmatize_action)\n",
    "temp_df=graph_df.apply(lambda row: replace_chars(row), axis=1)\n",
    "real_char=pd.DataFrame(temp_df)\n",
    "real_char[['charA', 'charB']] = real_char[0].apply(pd.Series)\n",
    "real_char.drop(0, axis=1, inplace=True)\n",
    "graph_df[['char A', 'char B']] = real_char[['charA', 'charB']]\n",
    "graph_df = graph_df.merge(movies, how='left', on='Wikipedia movie ID', suffixes=('_graph', '_movies'))\n",
    "graph_df = graph_df.merge(characters_df, how='left', left_on=['Wikipedia movie ID', 'char A'], right_on=['Wikipedia movie ID', 'character_name'], suffixes=('', '_charA'))\n",
    "graph_df.columns = [f'{col}_charA' if col in characters_df.columns and col != 'Wikipedia movie ID' else col for col in graph_df.columns]\n",
    "graph_df = graph_df.merge(characters_df, how='left', left_on=['Wikipedia movie ID', 'char B'], right_on=['Wikipedia movie ID', 'character_name'], suffixes=('', '_charB'))\n",
    "graph_df.columns = [f'{col}_charB' if col in characters_df.columns and col != 'Wikipedia movie ID' else col for col in graph_df.columns]\n",
    "graph_df.drop_duplicates(['Wikipedia movie ID', 'char A', 'combined_action', 'char B'], inplace=True)\n",
    "graph_df.to_csv('Data\\\\final_merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering interaactions in different topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore how we cluster on the vocabulary of our interactions, using GloVe embedings through spacy to get different interactions topics.\n",
    "\n",
    "We start by defining our topics (centroids of our clusters), then we proceed to compute the set of words that have a bigger similarity with the centroid than a fixed threshold (0.5 here).\n",
    "We then one-hot encode all the rows where one of the words of the clusters are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "topics=['fight', 'crime', 'care', 'fear', 'love', 'reject', 'sadness', 'collaborate', 'rivalry', 'mentor']\n",
    "\n",
    "min_similarity = 0.5 # You can adjust this value based on your needs\n",
    "\n",
    "for word in topics:\n",
    "        centroid_vector = nlp(word).vector\n",
    "        word_vectors_list = list(word_vectors.values())\n",
    "        # Calculate cosine similarity with 'centroid_vector' for each word in 'vocab'\n",
    "        similarity_scores = cosine_similarity([centroid_vector], word_vectors_list)\n",
    "        # Get the indices and similarity scores of words within the minimum similarity\n",
    "        close_words_indices = np.where(similarity_scores > min_similarity)[1]\n",
    "        close_words_similarity = [(list(word_vectors.keys())[i], similarity_scores[0, i]) for i in close_words_indices]\n",
    "        # Sort the words based on similarity, from most similar to least\n",
    "        close_words_similarity.sort(key=lambda x: x[1], reverse=True)\n",
    "        # Extract the sorted words\n",
    "        close_words = [word for word, _ in close_words_similarity]\n",
    "        print(f\"{len(close_words)} similar to '{word}': {close_words}\")\n",
    "        graph_df[word] = graph_df['lemmatized_action'].apply(lambda x: 1 if any(word in x for word in close_words) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=\n",
    "data = graph_df[graph_df['Wikipedia movie ID']==id]\n",
    "plt.figure(figsize=(15, 15))\n",
    "G=nx.MultiDiGraph()\n",
    "\n",
    "def edge_center_coordinates(edge, curvature, pos):\n",
    "    start_x, start_y = pos[edge[0]]\n",
    "    end_x, end_y = pos[edge[1]]\n",
    "    \n",
    "    # Calculate the control point coordinates for the curved edge\n",
    "    control_x = 0.5 * (start_x + end_x) + curvature * (end_y - start_y)\n",
    "    control_y = 0.5 * (start_y + end_y) - curvature * (end_x - start_x)\n",
    "    \n",
    "    # Calculate the Bezier curve parameters\n",
    "    t = 0.5  # Midpoint of the curve\n",
    "    bx = (1 - t)**2 * start_x + 2 * (1 - t) * t * control_x + t**2 * end_x\n",
    "    by = (1 - t)**2 * start_y + 2 * (1 - t) * t * control_y + t**2 * end_y\n",
    "    \n",
    "    return bx, by\n",
    "\n",
    "for subj in data[\"char A\"].unique():\n",
    "    G.add_node(subj, color=\"lightblue\", node_size=2000)\n",
    "    i=1\n",
    "    for verb, obj in data[data[\"char A\"]==subj][[\"combined_action\", \"char B\"]].itertuples(index=False):\n",
    "        G.add_edge(subj, obj, label=verb, curvature=i/10)\n",
    "        i+=2\n",
    "\n",
    "pos = nx.circular_layout(G)\n",
    "\n",
    "# Draw nodes separately\n",
    "nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue')\n",
    "# Draw node labels if needed\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')\n",
    "\n",
    "# Draw the graph with curved edges based on the 'curvature' attribute\n",
    "for edge in G.edges(data=True):\n",
    "    edge_data = edge[2]\n",
    "    label = edge_data['label']\n",
    "    curvature = edge_data.get('curvature', 0.1)  # Default curvature if 'curvature' is not present\n",
    "    #draw edges\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=[(edge[0], edge[1])], connectionstyle=f'arc3,rad={curvature}', edge_color='black', width=2, alpha=0.7, label=label, arrows=True, arrowsize=50, arrowstyle='-|>')\n",
    "    center_coordinates = edge_center_coordinates(edge, curvature, pos)\n",
    "    plt.text(center_coordinates[0], center_coordinates[1], label, color='red', fontsize=8, ha='center', va='center', bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
